---
title: "Chapter 6 - Linear Model Selection and Regularization"
author: "Zhengting (Johnathan) He"
date: "2022/2/19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Best Subset Selection


```{r}
require(ISLR)
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary))
```


```{r}
Hitters <- na.omit(Hitters)
dim(Hitters)
sum(is.na(Hitters))
```


```{r}
require(leaps)
regfit.full <- regsubsets(Salary ~ ., Hitters)
summary(regfit.full)
```


```{r}
regfit.full <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
reg.summary <- summary(regfit.full)
```


```{r}
names(reg.summary)
reg.summary$rsq
```


```{r}
par(mfrow = c(2, 2))
plot(reg.summary$rss , xlab = "Number of Variables ", ylab = "RSS",
     type = "l")
plot(reg.summary$adjr2 , xlab = "Number of Variables ",
     ylab = "Adjusted RSq", type = "l")
```


```{r}
which.max(reg.summary$adjr2)
```


```{r}
plot(reg.summary$adjr2 , xlab = "Number of Variables ",
     ylab = "Adjusted RSq", type = "l")
points(11, reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)
```


```{r}
which.min(reg.summary$cp)
```


```{r}
plot(reg.summary$cp , xlab = "Number of Variables ", ylab = "Cp",
     type = "l")
points(10, reg.summary$cp [10], col = "red", cex = 2, pch = 20)
```


```{r}
which.min(reg.summary$bic)
```


```{r}
plot(reg.summary$bic , xlab = "Number of Variables ", ylab = "BIC",
     type = "l")
points(6, reg.summary$bic[6], col = "red", cex = 2, pch = 20)
```


```{r}
require(tidyverse)
c("r2", "adjr2", "Cp", "bic") %>% 
    map(~ plot(regfit.full, scale = .x))
```


```{r}
coef(regfit.full, 6)
```


# Forward and Backward Stepwise Selection


```{r}
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters , nvmax = 19,
                         method = "forward")
summary(regfit.fwd)
```


```{r}
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19,
                         method = "backward")
summary(regfit.bwd)
```


```{r}
list(regfit.full, regfit.fwd, regfit.bwd) %>% 
    map( ~ coef(.x, 7))
```


# Choosing Among Models Using the Validation Set Approach and Cross-Validation


```{r}
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), rep = TRUE)
test <- (!train)
```


```{r}
regfit.best <- regsubsets(Salary ~ ., data = Hitters[train , ],
                          nvmax = 19)
```


```{r}
test.mat <- model.matrix(Salary ~ ., data = Hitters[test , ])
```


```{r}
val.errors <- rep(NA, 19)
for(i in 1:19) {
    coefi <- coef(regfit.best, id = i)
    pred <- test.mat[, names(coefi)] * coefi
    val.errors[i] <- mean((Hitters$Salary[test] - pred)^2)
}
```


```{r}
val.errors
```


```{r}
which.min(val.errors)
```


```{r}
coef(regfit.best, 9)
```


```{r}
predict.regsubsets =function (object , newdata ,id ,...){
    form <- as.formula (object$call [[2]])
    mat <- model.matrix(form , newdata)
    coefi <- coef(object , id = id)
    xvars <- names(coefi)
    mat[, xvars] %*% coefi
}
```


```{r}
regfit.best <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
coef(regfit.best, 9)
```


```{r}
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace = TRUE)
cv.errors <- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))
```


```{r}
for(j in 1:k) {
    best.fit <- regsubsets(Salary ~ ., data = Hitters[folds != j,], nvmax = 19)
    for(i in 1:19) {
        pred <- predict(best.fit, Hitters[folds == j,], id = i)
        cv.errors[j,i] <- mean((Hitters$Salary[folds == j] - pred)^2)
    }
}
```


```{r}
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
```


```{r}
par(mfrow = c(1,1))
plot(mean.cv.errors, type = "b")
```


```{r}
reg.best <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
coef(reg.best, 11)
```

# Lab 2: Ridge Regression and the Lasso


```{r}
x <- model.matrix(Salary ~ ., Hitters)[,-1]
y <- Hitters$Salary
```


## 6.6.1 Ridge Regression


```{r}
require(glmnet)
grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)
```


```{r}
dim(coef(ridge.mod))
```


```{r}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
```


```{r}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```


```{r}
predict(ridge.mod, s = 50, type = "coefficients")[1:20,]
```


```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]
```


```{r}
ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12)
ridge.pred <- predict(ridge.mod, s = 4, newx = x[test,])
mean((ridge.pred - y.test)^2)
```


```{r}
mean((mean(y[train]) - y.test)^2)
```


```{r}
ridge.pred <- predict(ridge.mod, s = 1e10, newx = x[test,])
mean((ridge.pred - y.test)^2)
```


```{r}
ridge.pred <- predict(ridge.mod, s = 0, newx = x[test,])
mean((ridge.pred - y.test)^2)
```


```{r}
lm(y ~ x, subset = train)
predict(ridge.mod, x = x, y = y, s = 0, exact = T, type = "coefficients")[1:20,]
```


```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 0)
plot(cv.out)
```


```{r}
bestlam <- cv.out$lambda.min
bestlam
```


```{r}
ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)
```


```{r}
out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20,]
```


## 6.6.2 The Lasso


```{r}
lasso.mod <- glmnet(x[train,] ,y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```


```{r}
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha = 1)
plot(cv.out)
```


```{r}
bestlam <- cv.out$lambda.min
lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test,])
mean((lasso.pred - y.test)^2)
```


```{r}
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20,]
lasso.coef
```


```{r}
lasso.coef[lasso.coef != 0]
```


# Lab 3: PCR and PLS Regression


## 6.7.1 Principal Components Regression


```{r}
require(pls)
set.seed(2)
pcr.fit <- pcr(Salary ~ ., data = Hitters, scale = TRUE, validation = "CV")
```


```{r}
summary(pcr.fit)
```


```{r}
validationplot(pcr.fit, val.type = "MSEP")
```


```{r}
# set.seed(1)
# pcr.fit <- pcr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")
# validationplot(pcr.fit, val.type = "MESP")
```


```{r}
# pcr.pred <- predict(pcr.fot, x[test,], ncomp = 7)
# mean((pcr.pred - y.test)^2)
```


```{r}
pcr.fit <- pcr(y ~ x, scale = TRUE, ncomp = 7)
summary(pcr.fit)
```


## 6.7.2 Partial Least Squares


```{r}
# set.seed(1)
# pls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, scale = TRUE, validation = "CV")
# summary(pls.fit)
# validationplot(pls.fit, val.type = "MSEP")
```


```{r}
# pls.pred <- predict(pls.fit, x[test,], ncomp = 2)
# mean((pls.pred - y.test)^2)
```


```{r}
pls.fit <- plsr(Salary ~ ., data = Hitters, scale = TRUE, ncomp = 2)
summary(pls.fit)
```

